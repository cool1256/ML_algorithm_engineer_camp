# 线性回归方法简介

## 形式化定义
+ 假设函数（hypotheses function）  
$$h_{\theta}(x)=\sum_{i=0}^{n} \theta_{i} x_{i}=\theta^{T} x$$  其中  x_{0}=1 
+ 损失函数（loss function）  
$$L(\theta)=\left(h_{\theta}(x)-y\right)^{2}$$
+ 代价函数（cost function）  
$$J(\theta)=\frac{1}{2 \mathrm{m}} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$$

# 梯度下降法（Gradient Descent）
+ 随机初始化&\theta&
+ 设置步长&\alpha&, 设置迭代次数&\m&
+ 求&J(\theta)&的导数$\nabla J(\theta)$
+ $\begin{array}{l}
\text { for } i=0 \text { to } m \text { : } \\
\qquad \theta:=\theta-\alpha \nabla J(\theta)
\end{array}$

# 使用梯度下降法求解线性回归问题
+ 代价函数
$$J(\theta)=\frac{1}{2 \mathrm{m}} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$$
+ 求解
$$\displaylines{ \begin{array}{l}
\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \\
=2 * \frac{1}{2 m} \sum_{i=1}^{m}\left[\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \frac{\partial}{\partial \theta_{j}}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)\right] \\
=\frac{1}{m} \sum_{i=1}^{m}\left[\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \frac{\partial}{\partial \theta_{j}}\left(\sum_{f=0}^{n} \theta_{f} x_{f}^{(i)}-y^{(i)}\right)\right] \\
=\frac{1}{m} \sum_{i=1}^{m}\left[\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\right] \\
\theta_{j}:=\theta_{j}+\alpha \frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}
\end{array}}$$

# 梯度下降类型
## 批量梯度下降（Batch Gradient Descent, BGD）
$$\theta_{j}:=\theta_{j}+\alpha \frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}$$
## 随机梯度下降(Stochastic Gradient Descent, SGD)
$$\theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}$$
## 小批量梯度下降(Mini-Batch Gradient Descent, MBGD)
+ 每次取batch-size个样本进行更新
$$\theta_{j}:=\theta_{j}+\frac{\alpha}{\text { batch-num }} \sum_{i=1}^{\text {bact }}\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}$$

# 评价指标
## 均方误差（MSE）
$$M S E=\frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)^{2}$$
## 均方根误差（RMSE）
$$R M S E=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-\hat{y}^{(j)}\right)^{2}}$$
## 平均绝对误差（MAE）
$$M A E=\frac{1}{m} \sum_{i=1}^{m}\left|y^{(i)}-\hat{y}^{(i)}\right|$$

# 欠拟合与过拟合
![欠拟合和过拟合](./img/2_1.png)

# [正则化](https://blog.csdn.net/qq_20412595/article/details/81636105)
+ L2范数正则化解决过拟合（Ridge Regression, 岭回归）
$$J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(0)}\right)-y^{(i)}\right)^{2}+\lambda\|\theta\|_{2}^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2} \quad(\lambda>0)$$
+ L1范数正则化解决过拟合（LASSO回归）
$$J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda|\theta|_{1}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n}\left|\theta_{j}\right|$$
+ L1与L2结合解决过拟合（Elastic Net, 弹性网）
$$J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda\left(\rho \mid \theta_{1}+(1-\rho)\|\theta\|_{2}^{2}\right) \quad(\lambda>0,0 \leq \rho \leq 1)$$

# Ridge回归(L2)求解与代码实现
+ 目标函数
$$\displaylines{ J(\theta)=\frac{1}{2 \mathrm{m}} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda\|\theta\|_{2}^{2} \\ =\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(0)}\right)-y^{(0)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta}$$
+ 岭回归求解
$$\displaylines{\begin{array}{l}
\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(j)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2} \\
=\frac{1}{m} \sum_{i=1}^{m}\left[\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(n)}\right]+2 \lambda \theta_{j}
\end{array}}$$
+ 迭代公式
$$\theta_{j}:=\theta_{j}+\alpha \frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}-2 \lambda \theta_{j}$$

